{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90436c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 16:46:07.060700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# This file is used to split the segmented videos into clips.!!!!!!!!!!!!!!!!!!\n",
    "# Also last code is used to convert the clips into train-test-validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling3D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Conv3D, MaxPooling3D, BatchNormalization,AveragePooling3D , concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feab3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths =['Converted/10.mp4','Converted/20.mp4','Converted/30.mp4','Converted/40.mp4','Converted/50.mp4','Converted/60.mp4','Converted/70.mp4']\n",
    "video_path=video_paths[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55570098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34363248/'H264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties from the original video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_file = 'Optical_flow/OP_30_n.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'H264')  # Use 'H264' for MP4 format\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(\n",
    "    winSize=(15, 15),\n",
    "    maxLevel=2,\n",
    "    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    ")\n",
    "\n",
    "# Initialize variables\n",
    "prev_gray = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_gray is not None:\n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, pyr_scale=0.5, levels=3, winsize=15,\n",
    "                                            iterations=3, poly_n=5, poly_sigma=1.2,\n",
    "                                            flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "\n",
    "        # Calculate magnitude of optical flow vectors\n",
    "        magnitude = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)\n",
    "\n",
    "        # Highlight the regions of optical flow velocity\n",
    "        threshold = 1.5\n",
    "        flow_threshold = np.where(magnitude > threshold, 255, 0).astype(np.uint8)\n",
    "\n",
    "        contours, _ = cv2.findContours(flow_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        farme_with_border = frame.copy()\n",
    "        cv2.drawContours(farme_with_border ,contours, -1,(0,0,255),2)#red border with border size 2\n",
    "        #flow_bgr = cv2.merge((flow_threshold, flow_threshold, flow_threshold))\n",
    "\n",
    "        #output_frame = cv2.addWeighted(frame, 0.7, flow_bgr, 0.3, 0)\n",
    "\n",
    "        cv2.imshow('Optical Flow Visualization', farme_with_border)\n",
    "\n",
    "        out.write(farme_with_border)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    prev_gray = gray\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55a7c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34363248/'H264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties from the original video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_file = 'Optical_flow/OP_50.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'H264')  # Use 'H264' for MP4 format\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(\n",
    "    winSize=(15, 15),\n",
    "    maxLevel=2,\n",
    "    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    ")\n",
    "\n",
    "# Initialize variables\n",
    "prev_gray = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_gray is not None:\n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, pyr_scale=0.5, levels=5, winsize=15,\n",
    "                                            iterations=5, poly_n=5, poly_sigma=1.2,\n",
    "                                            flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "\n",
    "        # Calculate magnitude of optical flow vectors\n",
    "        magnitude = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)\n",
    "\n",
    "        # Highlight the regions of optical flow velocity\n",
    "        threshold = 1.6\n",
    "        flow_threshold = np.where(magnitude > threshold, 255, 0).astype(np.uint8)\n",
    "\n",
    "        contours, _ = cv2.findContours(flow_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        mask = np.zeros_like(frame)\n",
    "        \n",
    "        cv2.drawContours(mask, contours, -1,(255,0,255), thickness=cv2.FILLED)\n",
    "        \n",
    "        object_inside_contour = cv2.bitwise_and(frame,mask)\n",
    "        \n",
    "        combined_frame = np.hstack((frame, object_inside_contour))\n",
    "        \n",
    "        average_velocity = np.mean(magnitude[np.where(magnitude>threshold)])\n",
    "        \n",
    "        cv2.putText(combined_frame, f'Average Velocity : {average_velocity:.3f}',\n",
    "                   (1316,30),cv2.FONT_HERSHEY_SIMPLEX, 1,(0,255,0),2,cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('Optical Flow Visualization', cv2.resize(combined_frame,(896,448)))\n",
    "\n",
    "        #out.write(object_inside_contour)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    prev_gray = gray\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa99b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server_admin/anaconda3/envs/soumya_2002/lib/python3.11/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the directory containing the dataset\n",
    "data_dir = 'Optical_Flow_Threshold3/'\n",
    "\n",
    "# Define the directory to store the clips\n",
    "output_dir = 'Klips/'\n",
    "\n",
    "# Define the list of classes (folder names) in the dataset\n",
    "classes = ['10', '20', '30', '40', '50', '60', '70']\n",
    "\n",
    "# Function to create directories if they don't exist\n",
    "def create_directories():\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "# Function to load video clips and their labels from the dataset directory\n",
    "def load_video_clips(data_dir, classes, num_frames=16, stride=16):\n",
    "    video_clips = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        video_files = glob.glob(os.path.join(class_dir, '*.mp4'))\n",
    "\n",
    "        for video_file in video_files:\n",
    "            # Read video frames\n",
    "            frames, _, _ = torchvision.io.read_video(video_file)\n",
    "            num_frames_video = frames.shape[0]\n",
    "\n",
    "            # Create video clips with the specified stride\n",
    "            for i, start in enumerate(range(0, num_frames_video - num_frames + 1, stride)):\n",
    "                end = start + num_frames\n",
    "                clip = frames[start:end]\n",
    "                video_clips.append(clip)\n",
    "                labels.append(int(class_name))\n",
    "\n",
    "                # Save the clip to the output directory\n",
    "                output_class_dir = os.path.join(output_dir, class_name)\n",
    "                output_file = os.path.join(output_class_dir, f'clip{i + 1}.mp4')  # You can replace '.xyz' with the appropriate video format\n",
    "                fps = 30\n",
    "                torchvision.io.write_video(output_file, clip, fps = fps)\n",
    "\n",
    "    return video_clips, labels\n",
    "\n",
    "# Create the output directories\n",
    "create_directories()\n",
    "\n",
    "# Load video clips and their labels\n",
    "video_clips, labels = load_video_clips(data_dir, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1be041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the directory containing the dataset\n",
    "data_dir = 'Optical_Flow_Threshold_3/'\n",
    "\n",
    "# Define the directory to store the clips\n",
    "output_dir = 'Llip/'\n",
    "\n",
    "# Define the list of classes (folder names) in the dataset\n",
    "classes = ['10', '20', '30', '40', '50', '60', '70']\n",
    "\n",
    "# Function to create directories if they don't exist\n",
    "def create_directories():\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "# Function to load video clips and their labels from the dataset directory\n",
    "def load_video_clips(data_dir, classes, num_frames=16, stride=8):\n",
    "    video_clips = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        video_files = glob.glob(os.path.join(class_dir, '*.mp4'))\n",
    "\n",
    "        for video_file in video_files:\n",
    "            # Read video frames\n",
    "            frames, _, _ = torchvision.io.read_video(video_file)\n",
    "            num_frames_video = frames.shape[0]\n",
    "\n",
    "            # Create video clips with the specified stride\n",
    "            for i, start in enumerate(range(0, num_frames_video - num_frames + 1, stride)):\n",
    "                end = start + num_frames\n",
    "                clip = frames[start:end]\n",
    "                video_clips.append(clip)\n",
    "                labels.append(int(class_name))\n",
    "\n",
    "                # Save the clip to the output directory\n",
    "                output_class_dir = os.path.join(output_dir, class_name)\n",
    "                output_file = os.path.join(output_class_dir, f'clip{i + 1+ 1069}.mp4')  # You can replace '.xyz' with the appropriate video format\n",
    "                fps = 5\n",
    "                torchvision.io.write_video(output_file, clip, fps=fps)\n",
    "\n",
    "            # Clear the lists to free up memory after processing each class\n",
    "            video_clips.clear()\n",
    "            labels.clear()\n",
    "\n",
    "    return video_clips, labels\n",
    "\n",
    "# Create the output directories\n",
    "create_directories()\n",
    "\n",
    "# Load video clips and their labels\n",
    "video_clips, labels = load_video_clips(data_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1369e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the directory containing the dataset\n",
    "data_dir = 'Unseen_Data/'\n",
    "\n",
    "# Define the directory to store the clips\n",
    "output_dir = 'Unseen/'\n",
    "\n",
    "# Define the list of classes (folder names) in the dataset\n",
    "classes = ['10', '20', '30', '40', '50', '60', '70']\n",
    "\n",
    "# Function to create directories if they don't exist\n",
    "def create_directories():\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "# Function to load video clips and their labels from the dataset directory\n",
    "def load_video_clips(data_dir, classes, num_frames=16, stride=8):\n",
    "    video_clips = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        video_files = glob.glob(os.path.join(class_dir, '*.mp4'))\n",
    "\n",
    "        for video_file in video_files:\n",
    "            # Read video frames\n",
    "            frames, _, _ = torchvision.io.read_video(video_file)\n",
    "            num_frames_video = frames.shape[0]\n",
    "\n",
    "            # Create video clips with the specified stride\n",
    "            for i, start in enumerate(range(0, num_frames_video - num_frames + 1, stride)):\n",
    "                end = start + num_frames\n",
    "                clip = frames[start:end]\n",
    "                video_clips.append(clip)\n",
    "                labels.append(int(class_name))\n",
    "\n",
    "                # Save the clip to the output directory\n",
    "                output_class_dir = os.path.join(output_dir, class_name)\n",
    "                output_file = os.path.join(output_class_dir, f'clip{i +1}.mp4')  # You can replace '.xyz' with the appropriate video format\n",
    "                fps = 5\n",
    "                torchvision.io.write_video(output_file, clip,fps=fps)\n",
    "\n",
    "            # Clear the lists to free up memory after processing each class\n",
    "            video_clips.clear()\n",
    "            labels.clear()\n",
    "\n",
    "    return video_clips, labels\n",
    "\n",
    "# Create the output directories\n",
    "create_directories()\n",
    "\n",
    "# Load video clips and their labels\n",
    "video_clips, labels = load_video_clips(data_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb54b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e46670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the directory containing the clips\n",
    "klips_dir = 'Llips/'\n",
    "\n",
    "# Define the list of classes (class folder names)\n",
    "classes = ['10', '20', '30', '40', '50', '60', '70']\n",
    "\n",
    "# Function to split clips into train, validation, and test sets for each class\n",
    "def split_clips_by_class(class_dir, train_size=0.7, test_size=0.15, validation_size=0.15, random_seed=None):\n",
    "    clip_files = os.listdir(class_dir)\n",
    "    \n",
    "    # Split clips into train, validation, and test sets\n",
    "    train_clips, temp_clips = train_test_split(clip_files, train_size=train_size, random_state=random_seed)\n",
    "    validation_test_clips = train_test_split(temp_clips, test_size=test_size / (1 - train_size), random_state=random_seed)\n",
    "    validation_clips, test_clips = validation_test_clips[0], validation_test_clips[1]\n",
    "    \n",
    "    return train_clips, validation_clips, test_clips\n",
    "\n",
    "# Function to move clips to their respective subdirectories\n",
    "def move_clips_to_split(class_dir, split_dir, clip_files):\n",
    "    for clip_file in clip_files:\n",
    "        src_path = os.path.join(class_dir, clip_file)\n",
    "        dst_path = os.path.join(split_dir, clip_file)\n",
    "        os.rename(src_path, dst_path)\n",
    "\n",
    "# Create 'Train', 'Validation', and 'Test' directories\n",
    "train_dir = os.path.join(klips_dir, 'Train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "validation_dir = os.path.join(klips_dir, 'Validation')\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(klips_dir, 'Test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each class folder and split clips\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(klips_dir, class_name)\n",
    "    \n",
    "    # Create class-specific subdirectories in 'Train', 'Validation', and 'Test'\n",
    "    class_train_dir = os.path.join(train_dir, class_name)\n",
    "    os.makedirs(class_train_dir, exist_ok=True)\n",
    "    \n",
    "    class_validation_dir = os.path.join(validation_dir, class_name)\n",
    "    os.makedirs(class_validation_dir, exist_ok=True)\n",
    "    \n",
    "    class_test_dir = os.path.join(test_dir, class_name)\n",
    "    os.makedirs(class_test_dir, exist_ok=True)\n",
    "    \n",
    "    # Split clips for the current class\n",
    "    clip_files = os.listdir(class_dir)\n",
    "    train_clips, validation_clips, test_clips = split_clips_by_class(class_dir, train_size=0.7, test_size=0.15, validation_size=0.15, random_seed=42)\n",
    "    \n",
    "    # Move clips to their respective subdirectories\n",
    "    move_clips_to_split(class_dir, class_train_dir, train_clips)\n",
    "    move_clips_to_split(class_dir, class_validation_dir, validation_clips)\n",
    "    move_clips_to_split(class_dir, class_test_dir, test_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045d1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
